{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning using PySpark - Working in Progress\n",
    "#### Tiancheng Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a data cleaning script without \"import pandas as pd\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "from datetime import datetime as Date\n",
    "from pyspark.sql.dataframe import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a SparkSession\n",
    "#.master('local[*]') specifies to use all cores on local machine\n",
    "spark = SparkSession.builder \\\n",
    "                    .master('local[*]') \\\n",
    "                    .config(\"spark-master\", \"local\") \\\n",
    "                    .appName('INSY695') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we specify the schema before loading the data, in this case it doesn't need to\n",
    "#detect the data type which makes the loading faster\n",
    "\n",
    "#Note that we load the date and time as strings and transform them\n",
    "#back to date and time once we dealt with the NUlls\n",
    "#since isnan() and col().isNull() doesn't seem to work with date\n",
    "bpd_schema = StructType([\n",
    "    # Define the name field\n",
    "    StructField('CrimeDate', StringType(), True), # Add the CrimeDate field\n",
    "    StructField('CrimeTime', StringType(), False), # Add the CrimeTime field\n",
    "    StructField('CrimeCode', StringType(), False), # Add the CrimeCode field\n",
    "    StructField('Location', StringType(), False), # Add the Location field\n",
    "    StructField('Description', StringType(), False), # Add the Description field\n",
    "    StructField('Inside/Outside', StringType(), False), # Add the Inside/Outside field\n",
    "    StructField('Weapon', StringType(), False), # Add the Weapon field\n",
    "    StructField('Post', FloatType(), False), # Add the Post field\n",
    "    StructField('District', StringType(), False), # Add the District field\n",
    "    StructField('Neighborhood', StringType(), False), # Add the Neighborhood field\n",
    "    StructField('Longitude', FloatType(), False), # Add the Longitude field\n",
    "    StructField('Latitude', FloatType(), False), # Add the Latitude field\n",
    "    StructField('Location 1', StringType(), False), # Add the Location 1 field\n",
    "    StructField('Premise', StringType(), False), # Add the Premise field\n",
    "    StructField('Total Incidents', IntegerType(), False) # Add the Total Incidents field\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd_df = spark.read.csv('./DATA/BPD_CRIME_DATA.csv', header=True, dateFormat=\"dd/MM/yyyy\", schema=bpd_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CrimeDate</th><th>CrimeTime</th><th>CrimeCode</th><th>Location</th><th>Description</th><th>Inside/Outside</th><th>Weapon</th><th>Post</th><th>District</th><th>Neighborhood</th><th>Longitude</th><th>Latitude</th><th>Location 1</th><th>Premise</th><th>Total Incidents</th></tr>\n",
       "<tr><td>09/02/2017</td><td>23:30:00</td><td>3JK</td><td>4200 AUDREY AVE</td><td>ROBBERY - RESIDENCE</td><td>I</td><td>KNIFE</td><td>913.0</td><td>SOUTHERN</td><td>Brooklyn</td><td>-76.60541</td><td>39.22951</td><td>(39.2295100000, -...</td><td>ROW/TOWNHO</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>23:00:00</td><td>7A</td><td>800 NEWINGTON AVE</td><td>AUTO THEFT</td><td>O</td><td>null</td><td>133.0</td><td>CENTRAL</td><td>Reservoir Hill</td><td>-76.63217</td><td>39.3136</td><td>(39.3136000000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>22:53:00</td><td>9S</td><td>600 RADNOR AV</td><td>SHOOTING</td><td>Outside</td><td>FIREARM</td><td>524.0</td><td>NORTHERN</td><td>Winston-Govans</td><td>-76.60697</td><td>39.34768</td><td>(39.3476800000, -...</td><td>Street</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>22:50:00</td><td>4C</td><td>1800 RAMSAY ST</td><td>AGG. ASSAULT</td><td>I</td><td>OTHER</td><td>934.0</td><td>SOUTHERN</td><td>Carrollton Ridge</td><td>-76.64526</td><td>39.28315</td><td>(39.2831500000, -...</td><td>ROW/TOWNHO</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>22:31:00</td><td>4E</td><td>100 LIGHT ST</td><td>COMMON ASSAULT</td><td>O</td><td>HANDS</td><td>113.0</td><td>CENTRAL</td><td>Downtown West</td><td>-76.61365</td><td>39.28756</td><td>(39.2875600000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>22:00:00</td><td>5A</td><td>CHERRYCREST RD</td><td>BURGLARY</td><td>I</td><td>null</td><td>922.0</td><td>SOUTHERN</td><td>Cherry Hill</td><td>-76.62131</td><td>39.24867</td><td>(39.2486700000, -...</td><td>ROW/TOWNHO</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>21:15:00</td><td>1F</td><td>3400 HARMONY CT</td><td>HOMICIDE</td><td>Outside</td><td>FIREARM</td><td>232.0</td><td>SOUTHEASTERN</td><td>Canton</td><td>-76.56827</td><td>39.28202</td><td>(39.2820200000, -...</td><td>Street</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>21:35:00</td><td>3B</td><td>400 W LANVALE ST</td><td>ROBBERY - STREET</td><td>O</td><td>null</td><td>123.0</td><td>CENTRAL</td><td>Upton</td><td>-76.62789</td><td>39.30254</td><td>(39.3025400000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>21:00:00</td><td>4C</td><td>2300 LYNDHURST AVE</td><td>AGG. ASSAULT</td><td>O</td><td>OTHER</td><td>641.0</td><td>NORTHWESTERN</td><td>Windsor Hills</td><td>-76.68365</td><td>39.3137</td><td>(39.3137000000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>21:00:00</td><td>4E</td><td>1200 N ELLWOOD AVE</td><td>COMMON ASSAULT</td><td>I</td><td>HANDS</td><td>332.0</td><td>EASTERN</td><td>Berea</td><td>-76.57419</td><td>39.30551</td><td>(39.3055100000, -...</td><td>ROW/TOWNHO</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>21:00:00</td><td>4C</td><td>2300 LYNDHURST AVE</td><td>AGG. ASSAULT</td><td>O</td><td>OTHER</td><td>641.0</td><td>NORTHWESTERN</td><td>Windsor Hills</td><td>-76.68365</td><td>39.3137</td><td>(39.3137000000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>20:56:00</td><td>3CF</td><td>3600 EDMONDSON AVE</td><td>ROBBERY - COMMERCIAL</td><td>I</td><td>FIREARM</td><td>844.0</td><td>SOUTHWESTERN</td><td>Edgewood</td><td>-76.67759</td><td>39.29402</td><td>(39.2940200000, -...</td><td>RETAIL/SMA</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>20:55:00</td><td>6C</td><td>5100 PARK HEIGHTS...</td><td>LARCENY</td><td>null</td><td>null</td><td>614.0</td><td>NORTHWESTERN</td><td>Central Park Heights</td><td>-76.67511</td><td>39.34861</td><td>(39.3486100000, -...</td><td>null</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>20:10:00</td><td>4C</td><td>3900 GWYNNS FALLS...</td><td>AGG. ASSAULT</td><td>O</td><td>OTHER</td><td>641.0</td><td>NORTHWESTERN</td><td>Windsor Hills</td><td>-76.68169</td><td>39.314</td><td>(39.3140000000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>20:00:00</td><td>6D</td><td>5500 SUMMERFIELD AVE</td><td>LARCENY FROM AUTO</td><td>O</td><td>null</td><td>444.0</td><td>NORTHEASTERN</td><td>Frankford</td><td>-76.5427</td><td>39.33288</td><td>(39.3328800000, -...</td><td>YARD</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>19:52:00</td><td>5D</td><td>2200 VAN DEMAN ST</td><td>BURGLARY</td><td>I</td><td>null</td><td>243.0</td><td>SOUTHEASTERN</td><td>Holabird Industri...</td><td>-76.53557</td><td>39.26533</td><td>(39.2653300000, -...</td><td>OTHER - IN</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>18:08:00</td><td>9S</td><td>1200 E LAFAYETTE AV</td><td>SHOOTING</td><td>Outside</td><td>FIREARM</td><td>343.0</td><td>EASTERN</td><td>Oliver</td><td>-76.60246</td><td>39.31038</td><td>(39.3103800000, -...</td><td>Street</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>18:08:00</td><td>1F</td><td>1200 E LAFAYETTE AV</td><td>HOMICIDE</td><td>Outside</td><td>FIREARM</td><td>343.0</td><td>EASTERN</td><td>Oliver</td><td>-76.60246</td><td>39.31038</td><td>(39.3103800000, -...</td><td>Street</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>18:16:00</td><td>4E</td><td>1000 N EUTAW ST</td><td>COMMON ASSAULT</td><td>O</td><td>HANDS</td><td>132.0</td><td>CENTRAL</td><td>Madison Park</td><td>-76.62256</td><td>39.30083</td><td>(39.3008300000, -...</td><td>STREET</td><td>1</td></tr>\n",
       "<tr><td>09/02/2017</td><td>18:00:00</td><td>6G</td><td>100 S BROADWAY</td><td>LARCENY</td><td>I</td><td>null</td><td>212.0</td><td>SOUTHEASTERN</td><td>Washington Hill</td><td>-76.5939</td><td>39.2902</td><td>(39.2902000000, -...</td><td>CONVENIENC</td><td>1</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+----------+---------+---------+--------------------+--------------------+--------------+-------+-----+------------+--------------------+---------+--------+--------------------+----------+---------------+\n",
       "| CrimeDate|CrimeTime|CrimeCode|            Location|         Description|Inside/Outside| Weapon| Post|    District|        Neighborhood|Longitude|Latitude|          Location 1|   Premise|Total Incidents|\n",
       "+----------+---------+---------+--------------------+--------------------+--------------+-------+-----+------------+--------------------+---------+--------+--------------------+----------+---------------+\n",
       "|09/02/2017| 23:30:00|      3JK|     4200 AUDREY AVE| ROBBERY - RESIDENCE|             I|  KNIFE|913.0|    SOUTHERN|            Brooklyn|-76.60541|39.22951|(39.2295100000, -...|ROW/TOWNHO|              1|\n",
       "|09/02/2017| 23:00:00|       7A|   800 NEWINGTON AVE|          AUTO THEFT|             O|   null|133.0|     CENTRAL|      Reservoir Hill|-76.63217| 39.3136|(39.3136000000, -...|    STREET|              1|\n",
       "|09/02/2017| 22:53:00|       9S|       600 RADNOR AV|            SHOOTING|       Outside|FIREARM|524.0|    NORTHERN|      Winston-Govans|-76.60697|39.34768|(39.3476800000, -...|    Street|              1|\n",
       "|09/02/2017| 22:50:00|       4C|      1800 RAMSAY ST|        AGG. ASSAULT|             I|  OTHER|934.0|    SOUTHERN|    Carrollton Ridge|-76.64526|39.28315|(39.2831500000, -...|ROW/TOWNHO|              1|\n",
       "|09/02/2017| 22:31:00|       4E|        100 LIGHT ST|      COMMON ASSAULT|             O|  HANDS|113.0|     CENTRAL|       Downtown West|-76.61365|39.28756|(39.2875600000, -...|    STREET|              1|\n",
       "|09/02/2017| 22:00:00|       5A|      CHERRYCREST RD|            BURGLARY|             I|   null|922.0|    SOUTHERN|         Cherry Hill|-76.62131|39.24867|(39.2486700000, -...|ROW/TOWNHO|              1|\n",
       "|09/02/2017| 21:15:00|       1F|     3400 HARMONY CT|            HOMICIDE|       Outside|FIREARM|232.0|SOUTHEASTERN|              Canton|-76.56827|39.28202|(39.2820200000, -...|    Street|              1|\n",
       "|09/02/2017| 21:35:00|       3B|    400 W LANVALE ST|    ROBBERY - STREET|             O|   null|123.0|     CENTRAL|               Upton|-76.62789|39.30254|(39.3025400000, -...|    STREET|              1|\n",
       "|09/02/2017| 21:00:00|       4C|  2300 LYNDHURST AVE|        AGG. ASSAULT|             O|  OTHER|641.0|NORTHWESTERN|       Windsor Hills|-76.68365| 39.3137|(39.3137000000, -...|    STREET|              1|\n",
       "|09/02/2017| 21:00:00|       4E|  1200 N ELLWOOD AVE|      COMMON ASSAULT|             I|  HANDS|332.0|     EASTERN|               Berea|-76.57419|39.30551|(39.3055100000, -...|ROW/TOWNHO|              1|\n",
       "|09/02/2017| 21:00:00|       4C|  2300 LYNDHURST AVE|        AGG. ASSAULT|             O|  OTHER|641.0|NORTHWESTERN|       Windsor Hills|-76.68365| 39.3137|(39.3137000000, -...|    STREET|              1|\n",
       "|09/02/2017| 20:56:00|      3CF|  3600 EDMONDSON AVE|ROBBERY - COMMERCIAL|             I|FIREARM|844.0|SOUTHWESTERN|            Edgewood|-76.67759|39.29402|(39.2940200000, -...|RETAIL/SMA|              1|\n",
       "|09/02/2017| 20:55:00|       6C|5100 PARK HEIGHTS...|             LARCENY|          null|   null|614.0|NORTHWESTERN|Central Park Heights|-76.67511|39.34861|(39.3486100000, -...|      null|              1|\n",
       "|09/02/2017| 20:10:00|       4C|3900 GWYNNS FALLS...|        AGG. ASSAULT|             O|  OTHER|641.0|NORTHWESTERN|       Windsor Hills|-76.68169|  39.314|(39.3140000000, -...|    STREET|              1|\n",
       "|09/02/2017| 20:00:00|       6D|5500 SUMMERFIELD AVE|   LARCENY FROM AUTO|             O|   null|444.0|NORTHEASTERN|           Frankford| -76.5427|39.33288|(39.3328800000, -...|      YARD|              1|\n",
       "|09/02/2017| 19:52:00|       5D|   2200 VAN DEMAN ST|            BURGLARY|             I|   null|243.0|SOUTHEASTERN|Holabird Industri...|-76.53557|39.26533|(39.2653300000, -...|OTHER - IN|              1|\n",
       "|09/02/2017| 18:08:00|       9S| 1200 E LAFAYETTE AV|            SHOOTING|       Outside|FIREARM|343.0|     EASTERN|              Oliver|-76.60246|39.31038|(39.3103800000, -...|    Street|              1|\n",
       "|09/02/2017| 18:08:00|       1F| 1200 E LAFAYETTE AV|            HOMICIDE|       Outside|FIREARM|343.0|     EASTERN|              Oliver|-76.60246|39.31038|(39.3103800000, -...|    Street|              1|\n",
       "|09/02/2017| 18:16:00|       4E|     1000 N EUTAW ST|      COMMON ASSAULT|             O|  HANDS|132.0|     CENTRAL|        Madison Park|-76.62256|39.30083|(39.3008300000, -...|    STREET|              1|\n",
       "|09/02/2017| 18:00:00|       6G|      100 S BROADWAY|             LARCENY|             I|   null|212.0|SOUTHEASTERN|     Washington Hill| -76.5939| 39.2902|(39.2902000000, -...|CONVENIENC|              1|\n",
       "+----------+---------+---------+--------------------+--------------------+--------------+-------+-----+------------+--------------------+---------+--------+--------------------+----------+---------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This line allows to display the Dataframe\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "bpd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We follow the same steps as we did in Cleaning_Part1.ipynb\n",
    "#### First, we count the NULL values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>CrimeDate</th><th>CrimeTime</th><th>CrimeCode</th><th>Location</th><th>Description</th><th>Inside/Outside</th><th>Weapon</th><th>Post</th><th>District</th><th>Neighborhood</th><th>Longitude</th><th>Latitude</th><th>Location 1</th><th>Premise</th><th>Total Incidents</th></tr>\n",
       "<tr><td>0</td><td>0</td><td>0</td><td>2207</td><td>0</td><td>10279</td><td>180952</td><td>224</td><td>80</td><td>2740</td><td>2204</td><td>2204</td><td>2204</td><td>10757</td><td>0</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------+---------+---------+--------+-----------+--------------+------+----+--------+------------+---------+--------+----------+-------+---------------+\n",
       "|CrimeDate|CrimeTime|CrimeCode|Location|Description|Inside/Outside|Weapon|Post|District|Neighborhood|Longitude|Latitude|Location 1|Premise|Total Incidents|\n",
       "+---------+---------+---------+--------+-----------+--------------+------+----+--------+------------+---------+--------+----------+-------+---------------+\n",
       "|        0|        0|        0|    2207|          0|         10279|180952| 224|      80|        2740|     2204|    2204|      2204|  10757|              0|\n",
       "+---------+---------+---------+--------+-----------+--------------+------+----+--------+------------+---------+--------+----------+-------+---------------+"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bpd_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in bpd_df.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We drop the columns with repeated information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['CrimeCode', 'Location', 'Post', 'District', 'Location 1', 'Total Incidents']\n",
    "bpd_df = bpd_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-43391cc1643c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Modify the data type of CrimeTime and rename to Crime_Time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mbpd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Crime_Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_unixtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munix_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CrimeTime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'HH:mm:ss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTimeType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbpd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrimeTime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TimeType' is not defined"
     ]
    }
   ],
   "source": [
    "#Modify the data type of CrimeDate and rename to Crime_Date\n",
    "bpd_df = bpd_df.withColumn('Crime_Date', F.from_unixtime(F.unix_timestamp('CrimeDate', 'dd/MM/yyyy')).cast(DateType()))\n",
    "bpd_df = bpd_df.drop(bpd_df.CrimeDate)\n",
    "\n",
    "#Modify the data type of CrimeTime and rename to Crime_Time\n",
    "bpd_df = bpd_df.withColumn('Crime_Time', F.from_unixtime(F.unix_timestamp('CrimeTime', 'HH:mm:ss')).cast(TimeType()))\n",
    "bpd_df = bpd_df.drop(bpd_df.CrimeTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpd_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "func =  udf(lambda x: datetime.strptime(x, 'dd/MM/yyyy'), DateType())\n",
    "\n",
    "bpd_df = bpd_df.withColumn('test', func(col('CrimeDate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-153-bb47505e6338>\", line 1, in <lambda>\nNameError: name 'datetime' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    448\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-153-bb47505e6338>\", line 1, in <lambda>\nNameError: name 'datetime' is not defined\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-153-bb47505e6338>\", line 1, in <lambda>\nNameError: name 'datetime' is not defined\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mmax_num_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             sock_info = self._jdf.getRowsToPython(\n\u001b[0;32m--> 463\u001b[0;31m                 max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 605, in main\n    process()\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 597, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 88, in <lambda>\n    return lambda *a: toInternal(f(*a))\n  File \"/Users/tuliprichard/anaconda3/lib/python3.7/site-packages/pyspark/python/lib/pyspark.zip/pyspark/util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-153-bb47505e6338>\", line 1, in <lambda>\nNameError: name 'datetime' is not defined\n"
     ]
    }
   ],
   "source": [
    "bpd_df.withColumn(\"CrimeDate\",bpd_df['CrimeDate'].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: +-------------------+\n|          CrimeDate|\n+-------------------+\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n+-------------------+\nonly showing top 20 rows\n of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-ec7e95f9fc98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrimeDate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrimeDate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dd/MM/yyyy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CrimeDate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbpd_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/functions.py\u001b[0m in \u001b[0;36mto_timestamp\u001b[0;34m(col, format)\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_timestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1193\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: +-------------------+\n|          CrimeDate|\n+-------------------+\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n|2017-02-09 00:00:00|\n+-------------------+\nonly showing top 20 rows\n of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "bpd_df.CrimeDate = bpd_df.select(F.to_timestamp(bpd_df.CrimeDate, 'dd/MM/yyyy').alias('CrimeDate'))\n",
    "bpd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.parquet('filename.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
